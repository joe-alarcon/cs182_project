{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from tokenizers.pre_tokenizers import Digits, Sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "rust_tok = tokenizer.backend_tokenizer\n",
    "rust_tok.pre_tokenizer = Sequence([Digits(individual_digits=True), rust_tok.pre_tokenizer])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# data\n",
    "DATA_DIR = \"../data/\"\n",
    "OUTPUT_DIR = \"./out\"\n",
    "\n",
    "FILE_PATH = \"\"\n",
    "\n",
    "# copied from ICL paper\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 12\n",
    "N_EMBD = 256\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# our own\n",
    "EPOCHS = 50\n",
    "LEARN_RATE = 1e-3\n",
    "SAVE_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# with open(DATA_DIR + FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = f.read().split(\"*\")\n",
    "#     NUM_CURRICULUM_STEPS = len(data)\n",
    "#     for i in range(len(data)):\n",
    "#         data[i] = [ln.strip().split(\";\")[0] for ln in data[i].split(\"\\n\")]\n",
    "\n",
    "with open(DATA_DIR + FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "    NUM_CURRICULUM_STEPS = 1\n",
    "    data = [ln.strip().split(\";\")[0] for ln in data.split(\"\\n\")]\n",
    "\n",
    "class BaseConversionDataset(Dataset):\n",
    "    def __init__(self, prompts, tokenizer, max_length=128):\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        self.tokenizer = tokenizer\n",
    "        for chosen_prompt in prompts:\n",
    "            if len(chosen_prompt) == 0:\n",
    "                continue\n",
    "            label_idx = -1\n",
    "            while str.isnumeric(chosen_prompt[label_idx]):\n",
    "                label_idx -= 1\n",
    "            label_idx += 1\n",
    "            prompt = chosen_prompt[:label_idx]\n",
    "            target_str = chosen_prompt[label_idx:]\n",
    "\n",
    "            # Apply truncation and padding here to the inputs\n",
    "            # before creating the tensors\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                target_str,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",  # Pad to maximum length\n",
    "                max_length=MAX_LENGTH,  # Set maximum length\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_inputs[\"input_ids\"]\n",
    "            p_ids_len = len(self.tokenizer.encode(prompt, add_special_tokens=True))\n",
    "            labels = [-100] * p_ids_len + input_ids[p_ids_len:]\n",
    "\n",
    "            input_ids_list.append(torch.tensor(input_ids, dtype=torch.long))\n",
    "            labels_list.append(torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "        self.input_ids_list = input_ids_list\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.input_ids_list[idx].to(device), \"labels\": self.labels_list[idx].to(device)}\n",
    "\n",
    "print(\"Get Data\")\n",
    "\n",
    "dataset = BaseConversionDataset(data, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=LEARN_RATE,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"Start Training\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from tokenizers.pre_tokenizers import Digits, Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model_checkpoint, test_data_file, separated_by_star = False, stop_early = False):\n",
    "    MODEL_FILEPATH = model_checkpoint\n",
    "    DATA_FILE_PATH = test_data_file\n",
    "    star = separated_by_star\n",
    "\n",
    "    # Load in model checkpoint\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_FILEPATH)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Setup tokenizer\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    rust_tok = tokenizer.backend_tokenizer\n",
    "    rust_tok.pre_tokenizer = Sequence([Digits(individual_digits=True), rust_tok.pre_tokenizer])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    # Open and read test data\n",
    "    with open(DATA_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        if not star:\n",
    "            data_preprocess = f.read()\n",
    "            data = [ln.strip().split(\";\")[0] for ln in data_preprocess.split(\"\\n\")]\n",
    "        else:\n",
    "            data_preprocess = f.read().split(\"*\")\n",
    "            for i in range(len(data_preprocess)):\n",
    "                data_preprocess[i] = [ln.strip().split(\";\")[0] for ln in data_preprocess[i].split(\"\\n\")]\n",
    "\n",
    "    losses = []\n",
    "    if star:\n",
    "        mse_per_in_context_length = []\n",
    "        losses_length_5 = []\n",
    "        expected_output_length_5 = []\n",
    "        actual_output_length_5 = []\n",
    "        for i in range(len(data_preprocess)):\n",
    "            if stop_early and (i == 6):\n",
    "                return losses_length_5, expected_output_length_5, actual_output_length_5\n",
    "            for prompt in data_preprocess[i]:\n",
    "                target_number = prompt.split(\"->\")[-1]\n",
    "                curr = \"\"\n",
    "                idx = len(prompt) - 1\n",
    "                while curr != \">\":\n",
    "                    curr = prompt[idx]\n",
    "                    prompt = prompt[:idx + 1] if curr == \">\" else prompt[:idx]\n",
    "                    idx -= 1\n",
    "                input_ids = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                ).to(device)\n",
    "\n",
    "                input_ids_tensor = input_ids['input_ids']\n",
    "\n",
    "                output_sequences = model.generate(\n",
    "                    input_ids=input_ids_tensor,\n",
    "                    max_new_tokens=50,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                # generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "                # print(generated_only_text, \" vs \", target_number)\n",
    "\n",
    "                prompt_length = input_ids_tensor.shape[-1]\n",
    "                generated_only_ids = output_sequences[0, prompt_length:]\n",
    "                generated_only_text = tokenizer.decode(generated_only_ids, skip_special_tokens=True)\n",
    "                mse_loss = (int(target_number) - int(generated_only_text))**2\n",
    "                losses.append(mse_loss)\n",
    "                if i == 5:\n",
    "                    losses_length_5.append(mse_loss)\n",
    "                    expected_output_length_5.append(int(target_number))\n",
    "                    actual_output_length_5.append(int(generated_only_text))\n",
    "            \n",
    "            avg_mse = np.mean(losses)\n",
    "            print(i, avg_mse)\n",
    "            mse_per_in_context_length.append(avg_mse)\n",
    "            print(mse_per_in_context_length)\n",
    "    else:\n",
    "        for prompt in data:\n",
    "            target_number = prompt.split(\"->\")[-1]\n",
    "            curr = \"\"\n",
    "            idx = len(prompt) - 1\n",
    "            while curr != \">\":\n",
    "                curr = prompt[idx]\n",
    "                prompt = prompt[:idx + 1] if curr == \">\" else prompt[:idx]\n",
    "                idx -= 1\n",
    "            input_ids = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            input_ids_tensor = input_ids['input_ids']\n",
    "\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=input_ids_tensor,\n",
    "                max_new_tokens=50,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            # print(generated_only_text, \" vs \", target_number)\n",
    "\n",
    "            prompt_length = input_ids_tensor.shape[-1]\n",
    "            generated_only_ids = output_sequences[0, prompt_length:]\n",
    "            generated_only_text = tokenizer.decode(generated_only_ids, skip_special_tokens=True)\n",
    "            mse_loss = (int(target_number) - int(generated_only_text))**2\n",
    "            losses.append(mse_loss)\n",
    "\n",
    "        avg_mse = np.mean(losses)\n",
    "    \n",
    "    if star:\n",
    "        return mse_per_in_context_length\n",
    "    return losses, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = \"../model_checkpoints/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "gpt2_test = evaluate_model(model_checkpoint = \"gpt2\", \n",
    "                           test_data_file= DATA_DIR + \"test_ood_length.txt\", \n",
    "                           separated_by_star=True)\n",
    "\n",
    "mse_model_5_one_to_15_loss = evaluate_model(model_checkpoint=CHECKPOINTS_DIR + \"gpt2-len5-arbitrary-decimal/checkpoint-1000\",\n",
    "                                            test_data_file=DATA_DIR+\"test_ood_length.txt\",\n",
    "                                            separated_by_star = True)\n",
    "\n",
    "mse_model_mixture_one_to_15_loss = evaluate_model(model_checkpoint = CHECKPOINTS_DIR + \"gpt2-mixture-arbitrary-decimal/checkpoint-1500\",\n",
    "                                                  test_data_file=DATA_DIR+\"test_ood_length.txt\",\n",
    "                                                  separated_by_star=True)\n",
    "\n",
    "mse_model_mixture_integer_noise_loss = evaluate_model(model_checkpoint=CHECKPOINTS_DIR+\"gpt2-mixture-arbitrary-decimal/checkpoint-1500\",\n",
    "                                                      test_data_file=DATA_DIR+\"test_int_noise.txt\",\n",
    "                                                      separated_by_star = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model trained on length 5: \", mse_model_5_one_to_15_loss, \"\\n\",\n",
    "      \"model trained on mixture icl examples: \", mse_model_mixture_one_to_15_loss, \"\\n\",\n",
    "      \"model trained on mixture integer noise :\", mse_model_mixture_integer_noise_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model_mixture_ood_numbers = evaluate_model(model_checkpoint=CHECKPOINTS_DIR+\"gpt2-mixture-arbitrary-decimal/checkpoint-1500\",\n",
    "                                               test_data_file=DATA_DIR+\"test_ood_nums_1_to_15.txt\",\n",
    "                                               separated_by_star=True)\n",
    "\n",
    "mse_model_len5_ood_numbers = evaluate_model(model_checkpoint=CHECKPOINTS_DIR+\"gpt2-len5-arbitrary-decimal/checkpoint-1000\",\n",
    "                                               test_data_file=DATA_DIR+\"test_ood_nums_1_to_15.txt\",\n",
    "                                               separated_by_star=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, expected, output = evaluate_model(model_checkpoint=CHECKPOINTS_DIR+\"gpt2-len5-arbitrary-decimal/checkpoint-1000\",\n",
    "                                            test_data_file=DATA_DIR+\"test_ood_length.txt\",\n",
    "                                            separated_by_star=True,\n",
    "                                            stop_early=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was all the necessary code for training and evaluating the model. We add the code below as something else that we tried. The code below didn't work due to out of memory issues with cuda.\n",
    "\n",
    "# Alternative training approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from tokenizers.pre_tokenizers import Digits, Sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "rust_tok = tokenizer.backend_tokenizer\n",
    "rust_tok.pre_tokenizer = Sequence([Digits(individual_digits=True), rust_tok.pre_tokenizer])\n",
    "tokenizer.add_special_tokens({'pad_token': 'P'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "FILE_PATH = \"./data_curriculum.txt\"\n",
    "OUTPUT_DIR = \"./gpt2-arbitrary-decimal-cur\"\n",
    "\n",
    "# copied from ICL paper\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 12\n",
    "N_EMBD = 256\n",
    "\n",
    "\n",
    "# our own\n",
    "EPOCHS = 50\n",
    "LEARN_RATE = 1e-3\n",
    "SAVE_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_PER_STEP = 4\n",
    "WARMUP_RATIO = 0.05\n",
    "\n",
    "\n",
    "print(PAD_ID)\n",
    "\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_in_curriculum_steps = f.read().split(\"*\")\n",
    "    NUM_CURRICULUM_STEPS = len(data_in_curriculum_steps)\n",
    "    for i in range(len(data_in_curriculum_steps)):\n",
    "        data_in_curriculum_steps[i] = [ln.strip().split(\";\")[0] for ln in data_in_curriculum_steps[i].split(\"\\n\")]\n",
    "data = data_in_curriculum_steps\n",
    "\n",
    "\"\"\"\n",
    "data = \n",
    "[\n",
    "    [All prompts of length 5],\n",
    "    [All prompts of length 10],\n",
    "    ...,\n",
    "    [All prompts of length 50]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "class BaseConversionDataset(Dataset):\n",
    "    def __init__(self, prompts, tokenizer, number_of_curriculum_steps):\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_curriculum_step = number_of_curriculum_steps\n",
    "        self.current_curriculum_step = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts[self.current_curriculum_step])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            chosen_prompt = self.prompts[self.current_curriculum_step][idx]\n",
    "        except IndexError:\n",
    "            chosen_prompt = self.prompts[self.max_curriculum_step-1][idx]\n",
    "        label_idx = -1\n",
    "        while str.isnumeric(chosen_prompt[label_idx]):\n",
    "            label_idx -= 1\n",
    "        label_idx += 1\n",
    "        prompt = chosen_prompt[:label_idx]\n",
    "        target_str = chosen_prompt[label_idx:]\n",
    "\n",
    "        p_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        # prompt is the entire prompt except the last number: \"1->1,10->2,...,11->\"\n",
    "        t_ids = self.tokenizer.encode(target_str, add_special_tokens=False)\n",
    "        # target_str is just the last number: \"3\"\n",
    "\n",
    "        input_ids = p_ids + t_ids\n",
    "        # mask everything except target tokens\n",
    "        labels = [-100] * len(p_ids) + t_ids\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "        # return input_ids, labels\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    input_ids = [x for x in input_ids]\n",
    "    labels    = [x for x in labels]\n",
    "    # pad sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=PAD_ID)\n",
    "    labels    = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    attention_mask = (input_ids != PAD_ID).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "dataset = BaseConversionDataset(data, tokenizer, NUM_CURRICULUM_STEPS)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARN_RATE)\n",
    "total_steps = len(dataloader) * EPOCHS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    step = (epoch-1) // EPOCHS_PER_STEP\n",
    "    dataset.current_curriculum_step = step\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.save_pretrained(f\"{OUTPUT_DIR}/{epoch}\")\n",
    "        tokenizer.save_pretrained(f\"{OUTPUT_DIR}/{epoch}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch:>2} — curriculum_step={step} — avg CE loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('/data/notebook_files/gpt2-len5-arbitrary-decimal/checkpoint-1000')\n",
    "FILE_PATH = \"/data_test.txt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "# rust_tok = tokenizer.backend_tokenizer\n",
    "# rust_tok.pre_tokenizer = Sequence([Digits(individual_digits=True), rust_tok.pre_tokenizer])\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_preprocess = f.read()\n",
    "    data = [ln.strip().split(\";\")[0] for ln in data_preprocess.split(\"\\n\")]\n",
    "\n",
    "losses = []\n",
    "\n",
    "for prompt in data:\n",
    "    target_number = prompt.split(\"->\")[-1]\n",
    "    curr = \"\"\n",
    "    idx = len(prompt) - 1\n",
    "    while curr != \">\":\n",
    "        curr = prompt[idx]\n",
    "        prompt = prompt[:idx]\n",
    "        idx -= 1\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids_tensor = input_ids['input_ids']\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids_tensor,\n",
    "        max_new_tokens=50,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    prompt_length = input_ids_tensor.shape[-1]\n",
    "    generated_only_ids = output_sequences[0, prompt_length:]\n",
    "    generated_only_text = tokenizer.decode(generated_only_ids, skip_special_tokens=True)\n",
    "    print(generated_only_text)\n",
    "    mse_loss = (int(target_number) - int(generated_only_text))**2\n",
    "\n",
    "    losses.append(mse_loss)\n",
    "\n",
    "avg_mse = np.mean(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
