{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = \"./gpt2_token_raw_model/checkpoints/run_1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "losses = []\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "config = GPT2Config(\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryToDecimalDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size=128):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        \n",
    "        self.examples = tokenizer(lines, truncation=True, padding=True, max_length=block_size, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.examples[idx]\n",
    "        y = x.clone()\n",
    "        return x, y\n",
    "\n",
    "dataset = BinaryToDecimalDataset(\"./gpt2_token_raw_model/10k_data.txt\", tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "learn_rate = 1e-3\n",
    "optimizer = AdamW(model.parameters(), lr=learn_rate)\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} has Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}\")\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"Checkpoint to {checkpoint_path}\")\n",
    "\n",
    "# with open(\"./gpt2_token_raw_model/training_losses.txt\", \"w\") as f:\n",
    "#     for l in losses:\n",
    "#         f.write(f\"{l}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
